{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided example: Building a Random Forest Classifier with Spark in Batch Mode\n",
    "In this checkpoint, you'll create your first model with Spark. We'll walk through building a random forest classifier, using the UCI HAR (human activity recognition) dataset, which contains data captured by smartphones as test subjects perform different activities like walking, sitting, and laying down.\n",
    "\n",
    "## Setup\n",
    "All the code for this curriculum module is available in [this GitHub repo](https://github.com/Thinkful-Ed/big-data-student-resources), which contains the Jupyter notebooks and datasets that you'll need for this assignment, and future walkthroughs. Clone this repo to your local machine. Begin by reading through the material in the curriculum for each walkthrough, and then move on to run the appropriate notebook in Colab.\n",
    "\n",
    "To get started, recall that you'll need to upload the relevant files to your Google Drive. To this end, upload the files inside `datasets/UCI_HAR` to the Colab Datasets folder and the `rfc_spark_batch_02_26_19.ipynb` notebook to the `Colab Notebooks` folder in your drive. After reading the background information below, work through the guided example in this notebook.\n",
    "\n",
    "## Working with Spark in batch mode\n",
    "We'll begin this exercise by using Spark in **batch mode**. This mode is most similar to the approach you've taken so far in this course: load a dataset, clean it, then run models and evaluate their performance.\n",
    "\n",
    "Spark's other mode — **streaming mode** — differs from batch mode in that, under streaming mode, you do not have access to all of the data. Instead, you ingest data into Spark over a period of time. As each data element arrives, you perform data science tasks (clean, classify, etc) on the new data, then archive the result. Don't worry about the details of this now - we'll cover it soon enough. For now, you only need to be aware that there are two modes we can work from in Spark.\n",
    "\n",
    "Spark works a little differently than the data science stack you've worked with so far. In that stack, you are writing Python code that executes immediately on your local device.\n",
    "\n",
    "Spark, on the other hand, lives outside the Python environment. We pass instructions to the Spark server, Spark performs the requested calculations and reports back to our notebook. This server paradigm allows us to create and test models locally, and then once we're ready to deploy to production, we simply point our Python code to a remote server that is configured to handle larger datasets.\n",
    "\n",
    "Spark uses the concept of a **pipeline** in which we configure a number of steps that are only executed when we tell the server to run. This pipeline approach allows us to configure models and run them through multiple steps more efficiently on big data.\n",
    "\n",
    "It's always a good idea to review the documentation for the modules, methods, and functions that we'll use. Everything we use here can be found in the Apache Spark documentation. You don't need to read the documentation in depth now, but you should refer to it when you come across a piece of code you're not familiar with, or when you're unsure how to proceed in a challenge.\n",
    "\n",
    "In particular, you should familiarize yourself with the:\n",
    "\n",
    "- [Spark Overview](https://spark.apache.org/docs/2.4.0/index.html)\n",
    "- [Python API Documentation](https://spark.apache.org/docs/2.4.0/api/python/index.html) which is known as **PySpark**.\n",
    "For our work in this checkpoint, it's also good to read about random forests in Spark [here](https://spark.apache.org/docs/2.4.0/ml-classification-regression.html#random-forests).\n",
    "\n",
    "## Background: the UCI HAR dataset\n",
    "The dataset for this checkpoint contains data from experiments on 30 subjects who wore a smartphone attached to their waist. They performed 6 activities:\n",
    "\n",
    "- walking\n",
    "- walking up stairs\n",
    "- walking down stairs\n",
    "- sitting\n",
    "- standing\n",
    "- lying down\n",
    "\n",
    "The activities were recorded with video, which enabled time-stamped correlation with motion values collected from the smartphones. In particular, 3-axial linear acceleration and 3-axial angular velocity were captured from the device's accelerometer and gyroscope. This resulted in a 561-value feature vector, plus its corresponding activity label.\n",
    "\n",
    "As you'll see in a moment when you go through the `rfc_spark_batch_02_26_19` notebook, our data for this exercise is in a file called `allData.csv`. This file contains the same data as the original UCI HAR dataset (available on internet) but cleaned up so it's ready to use in Spark.\n",
    "\n",
    "The original dataset is comprised of four files. There is training data, which contains one file for features and one for labels, and test data, which also contains one file for features and one for labels.\n",
    "\n",
    "Here's what we did to clean up the original data:\n",
    "\n",
    "1. The original source files are space-delimited and unfortunately, there is inconsistency in spacing. Most of the time a single space is used to delimit, but in some places, there are double spaces. If we were to import the original data with these inconsistencies, breaking on spaces, these double spaces would lead to extraneous columns and create problems for our classifier. Therefore, we removed all double delimiters and replaced spaces with commas. This second step was not absolutely necessary, but we prefer to use the more common CSV format.\n",
    "2. We merged the test and training feature data into a single final dataset. The labels are the same for both training and test data, so we created a single label file (`activity_labels.csv`).\n",
    "\n",
    "As you'll see in a moment, the final dataset ends up having 10,299 rows and 562 columns. All values are numeric. The labels are integers, and the features are doubles. Because the source data is already numeric, it's simpler to build and demonstrate our classifier in Spark.\n",
    "\n",
    "## Try it out\n",
    "With that context, you're ready to work through this exercise. Open the `rfc_spark_batch_02_26_19.ipynb` notebook with Colab, and closely read through it, executing each cell.\n",
    "\n",
    "Remember that to open a notebook in Colab from Google Drive, right-click on the file name and choose \"Open From -> Colaboratory:\"\n",
    "\n",
    "![colab_right_click](Photos/colab_right_click.png)\n",
    "\n",
    "At the end of the walkthrough, you'll find some suggestions for further exploration you can do on your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
