{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Supervised NLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " [The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the hea\n"
     ]
    }
   ],
   "source": [
    "bible = gutenberg.raw('bible-kjv.txt')\n",
    "melville = gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "print('\\nRaw:\\n', bible[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      " The First Book of Moses: Called Genesis In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. And God said, Let there be light: and there was light. And God saw the light, that it was good: and God divided the light from the darkness. And God called the li\n"
     ]
    }
   ],
   "source": [
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text) # Removing potential dashes\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text) # This pattern matches all text between square brackets.\n",
    "    text = re.sub(r'The Old Testament of the King James Bible','', text) # Removing the heading\n",
    "    text = re.sub(r'\\d+:\\d+ ','', text) # Removing all references\n",
    "    text = ' '.join(text.split()) # Removing excess white space\n",
    "    return text\n",
    "\n",
    "# Clean the Bible data.\n",
    "bible = text_cleaner(bible)\n",
    "\n",
    "# Clean the Milton data.\n",
    "melville = re.sub(r'VOLUME \\w+', '', melville)\n",
    "melville = re.sub(r'CHAPTER \\w+', '', melville)\n",
    "melville = text_cleaner(melville)\n",
    "\n",
    "# Print the first 400 characters of Bible \n",
    "print('Preview:\\n', bible[0:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bible: Done!\n",
      "Paradise: Done!\n"
     ]
    }
   ],
   "source": [
    "# Parse the cleaned corpuses\n",
    "nlp = spacy.load('en')\n",
    "bible_doc = nlp(bible[:25000])        # Restricting data size: original is over 3,000,000\n",
    "print('Bible: Done!')\n",
    "\n",
    "melville_doc = nlp(melville[:25000])      # Restricting data size: original is over 400,000\n",
    "print('Paradise: Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(The, First, Book, of, Moses, :, Called, Genesis)</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(In, the, beginning, God, created, the, heaven...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(And, the, earth, was, without, form, ,, and, ...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(And, the, Spirit, of, God, moved, upon, the, ...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(And, God, said, ,, Let, there, be, light, :, ...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0  (The, First, Book, of, Moses, :, Called, Genesis)  Bible\n",
       "1  (In, the, beginning, God, created, the, heaven...  Bible\n",
       "2  (And, the, earth, was, without, form, ,, and, ...  Bible\n",
       "3  (And, the, Spirit, of, God, moved, upon, the, ...  Bible\n",
       "4  (And, God, said, ,, Let, there, be, light, :, ...  Bible"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "bible_sents = [[sent, \"Bible\"] for sent in bible_doc.sents]\n",
    "melville_sents = [[sent, \"Milton\"] for sent in melville_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two corpuses into one data frame.\n",
    "sentences = pd.DataFrame(bible_sents + melville_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \"\"\"Utility function to create a list of the 2000 most common words.\"\"\"\n",
    "    allwords = [token.lemma_                    # Filter out punctuation and stop words.\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]     # Return the most common words.\n",
    "    \n",
    "def bow_features(sentences, common_words):\n",
    "    \"\"\"Create DataFrame with features for each word in common word set where value is count of appearances in sentence.\"\"\"\n",
    "    df = pd.DataFrame(columns=common_words)     # Scaffold the data frame\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0                 # Initiliaze counts to zero\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        words = [token.lemma_                   # Convert the sentence to lemmas\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct         # Filter punctuation, stop words, and uncommon words\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        for word in words:                      # Populate the row with word counts.\n",
    "            df.loc[i, word] += 1\n",
    "        if i % 50 == 0:                         # Counter to ensure kernel doesn't hang\n",
    "            print(\"Processing row {}\".format(i))\n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "biblewords = bag_of_words(bible_doc)\n",
    "melvillewords = bag_of_words(melville_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(biblewords + melvillewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asswage</th>\n",
       "      <th>wish</th>\n",
       "      <th>13</th>\n",
       "      <th>serpent</th>\n",
       "      <th>float</th>\n",
       "      <th>board</th>\n",
       "      <th>shady</th>\n",
       "      <th>limit</th>\n",
       "      <th>TOOKE</th>\n",
       "      <th>Mr.</th>\n",
       "      <th>...</th>\n",
       "      <th>extremity</th>\n",
       "      <th>content</th>\n",
       "      <th>sw</th>\n",
       "      <th>Heaven</th>\n",
       "      <th>true</th>\n",
       "      <th>picture</th>\n",
       "      <th>give</th>\n",
       "      <th>vent</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, First, Book, of, Moses, :, Called, Genesis)</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(In, the, beginning, God, created, the, heaven...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, the, earth, was, without, form, ,, and, ...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, the, Spirit, of, God, moved, upon, the, ...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, God, said, ,, Let, there, be, light, :, ...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  asswage wish 13 serpent float board shady limit TOOKE Mr.  ... extremity  \\\n",
       "0       0    0  0       0     0     0     0     0     0   0  ...         0   \n",
       "1       0    0  0       0     0     0     0     0     0   0  ...         0   \n",
       "2       0    0  0       0     0     0     0     0     0   0  ...         0   \n",
       "3       0    0  0       0     0     0     0     0     0   0  ...         0   \n",
       "4       0    0  0       0     0     0     0     0     0   0  ...         0   \n",
       "\n",
       "  content sw Heaven true picture give vent  \\\n",
       "0       0  0      0    0       0    0    0   \n",
       "1       0  0      0    0       0    0    0   \n",
       "2       0  0      0    0       0    0    0   \n",
       "3       0  0      0    0       0    0    0   \n",
       "4       0  0      0    0       0    0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (The, First, Book, of, Moses, :, Called, Genesis)       Bible  \n",
       "1  (In, the, beginning, God, created, the, heaven...       Bible  \n",
       "2  (And, the, earth, was, without, form, ,, and, ...       Bible  \n",
       "3  (And, the, Spirit, of, God, moved, upon, the, ...       Bible  \n",
       "4  (And, God, said, ,, Let, there, be, light, :, ...       Bible  \n",
       "\n",
       "[5 rows x 1682 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating features for sentences. Takes a few minutes to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421, 1680) (421,)\n",
      "Training set score:\t 0.9833729216152018\n",
      "Test set score:\t\t 0.9466192170818505\n"
     ]
    }
   ],
   "source": [
    "y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:\\t', lr.score(X_train, y_train))\n",
    "print('Test set score:\\t\\t', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>whale</td>\n",
       "      <td>1.017759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features  coefficients\n",
       "749    whale      1.017759"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_scores = pd.DataFrame()\n",
    "coef_scores['features'] = word_counts.drop(['text_sentence','text_source'], 1).columns\n",
    "coef_scores['coefficients'] = lr.coef_[0,:]\n",
    "\n",
    "coef_scores[coef_scores.coefficients == coef_scores.coefficients.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_paragraphs = gutenberg.paras('bible-kjv.txt')\n",
    "melville_paragraphs = gutenberg.paras('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_paras=[]\n",
    "melville_paras=[]\n",
    "\n",
    "def paragraph_split(raw_corpus, split_corpus):\n",
    "    \"\"\"Splits the paragraph corpus into a list of separate paragraphs.\"\"\"\n",
    "    for paragraph in raw_corpus:\n",
    "        para=paragraph[0]\n",
    "        para=[re.sub(r'--','',word) for word in para]\n",
    "        split_corpus.append(' '.join(para))\n",
    "\n",
    "paragraph_split(bible_paragraphs, bible_paras)   \n",
    "paragraph_split(melville_paragraphs, melville_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_df = [[para, \"Bible\"] for para in bible_paras]\n",
    "melville_df = [[para, \"Melville\"] for para in melville_paras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ The King James Bible ]</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Old Testament of the King James Bible</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The First Book of Moses : Called Genesis</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 : 1 In the beginning God created the heaven ...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 : 2 And the earth was without form , and voi...</td>\n",
       "      <td>Bible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph source\n",
       "0                           [ The King James Bible ]  Bible\n",
       "1          The Old Testament of the King James Bible  Bible\n",
       "2           The First Book of Moses : Called Genesis  Bible\n",
       "3  1 : 1 In the beginning God created the heaven ...  Bible\n",
       "4  1 : 2 And the earth was without form , and voi...  Bible"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the paragraphs from the two corpuses into one data frame.\n",
    "paragraphs = pd.DataFrame(bible_df + melville_df, columns=['paragraph','source'])\n",
    "paragraphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = paragraphs['source']\n",
    "X = list(paragraphs['paragraph'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predicted:  0.9620472584618192\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.50, # drop words in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Correction factor that treat long & short paragraphs equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies. Prevents divide-by-zero errors\n",
    "                            )\n",
    "feature_reduction = TruncatedSVD(1000)\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "text_clf = Pipeline([('vect', vectorizer),\n",
    "                     ('svd', feature_reduction),\n",
    "                     ('clf', classifier), ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "print(\"Accuracy of predicted: \", np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation for Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86524823 0.88652482 0.84397163 0.88571429 0.96402878]\n",
      "0.8890975487087533\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation for Logistic Regression of BoW\n",
    "y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "\n",
    "scores = cross_val_score(lr, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95274585 0.9649699  0.96734173 0.96678226 0.96568717]\n",
      "0.9635053799984477\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation for Logistic Regression pipeline using TF-IDF\n",
    "y = paragraphs['source']\n",
    "X = list(paragraphs['paragraph'])\n",
    "\n",
    "scores = cross_val_score(text_clf, X, y, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, our TF-IDF pipeline using the Logistic Regression model easily outperformed the other Logistic Regression using BoW. The model does take considerably longer to run; however, it ultimately is a more accurate model with considerably less variance among the cross-validation folds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
