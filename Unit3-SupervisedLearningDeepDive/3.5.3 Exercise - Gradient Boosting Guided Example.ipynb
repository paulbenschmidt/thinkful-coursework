{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost guided example\n",
    "\n",
    "Having walked through gradient boost by hand, now let's try it with SKlearn.  We'll still use the European Social Survey Data, but now with a categorical outcome: Whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now working with a binary outcome, we've switched to a classifier.  Now our loss function can't be the residuals.  Our options are \"deviance\", or \"exponential\".  Deviance is used for logistic regression, and we'll try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04650845608292417\n",
      "Percent Type II errors: 0.17607746863066012\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06257668711656442\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7622549  0.7120098  0.71691176 0.72760736 0.77272727 0.76904177\n",
      " 0.72727273 0.74447174 0.76412776 0.74692875]\n",
      "0.744335385712145\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike decision trees, gradient boost solutions are not terribly easy to interpret on the surface.  But they aren't quite a black box.  We can get a measure of how important various features are by counting how many times a feature is used over the course of many decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DRILL: Improve this gradient boost model\n",
    "\n",
    "While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement.  Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set.  Strategies you might use include:\n",
    "\n",
    "* Creating new features\n",
    "* Applying more overfitting-prevention strategies like subsampling\n",
    "* More iterations\n",
    "* Trying a different loss function\n",
    "* Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cntry</th>\n",
       "      <th>idno</th>\n",
       "      <th>year</th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>gndr</th>\n",
       "      <th>agea</th>\n",
       "      <th>partner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CH</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CH</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CH</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CH</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CH</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cntry  idno  year  tvtot  ppltrst  pplfair  pplhlp  happy  sclmeet  sclact  \\\n",
       "0    CH   5.0     6    3.0      3.0     10.0     5.0    8.0      5.0     4.0   \n",
       "1    CH  25.0     6    6.0      5.0      7.0     5.0    9.0      3.0     2.0   \n",
       "2    CH  26.0     6    1.0      8.0      8.0     8.0    7.0      6.0     3.0   \n",
       "3    CH  28.0     6    4.0      6.0      6.0     7.0   10.0      6.0     2.0   \n",
       "4    CH  29.0     6    5.0      6.0      7.0     5.0    8.0      7.0     2.0   \n",
       "\n",
       "   gndr  agea  partner  \n",
       "0   2.0  60.0      1.0  \n",
       "1   2.0  59.0      1.0  \n",
       "2   1.0  24.0      2.0  \n",
       "3   2.0  64.0      1.0  \n",
       "4   2.0  55.0      1.0  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For easy loading...\n",
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 of 5: Creating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cntry         6\n",
       "idno       3076\n",
       "year          2\n",
       "tvtot         8\n",
       "ppltrst      11\n",
       "pplfair      11\n",
       "pplhlp       11\n",
       "happy        11\n",
       "sclmeet       7\n",
       "sclact        5\n",
       "gndr          2\n",
       "agea         83\n",
       "partner       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the above, the year and gender are categorical variable. Therefore, we should make these features into dummies so that they aren't treated with incremental value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>agea</th>\n",
       "      <th>cntry_CH</th>\n",
       "      <th>cntry_CZ</th>\n",
       "      <th>cntry_DE</th>\n",
       "      <th>cntry_ES</th>\n",
       "      <th>cntry_NO</th>\n",
       "      <th>cntry_SE</th>\n",
       "      <th>male</th>\n",
       "      <th>second_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tvtot  ppltrst  pplfair  pplhlp  happy  sclmeet  sclact  agea  cntry_CH  \\\n",
       "0    3.0      3.0     10.0     5.0    8.0      5.0     4.0  60.0         1   \n",
       "1    6.0      5.0      7.0     5.0    9.0      3.0     2.0  59.0         1   \n",
       "2    1.0      8.0      8.0     8.0    7.0      6.0     3.0  24.0         1   \n",
       "3    4.0      6.0      6.0     7.0   10.0      6.0     2.0  64.0         1   \n",
       "4    5.0      6.0      7.0     5.0    8.0      7.0     2.0  55.0         1   \n",
       "\n",
       "   cntry_CZ  cntry_DE  cntry_ES  cntry_NO  cntry_SE  male  second_year  \n",
       "0         0         0         0         0         0     1            0  \n",
       "1         0         0         0         0         0     1            0  \n",
       "2         0         0         0         0         0     0            0  \n",
       "3         0         0         0         0         0     1            0  \n",
       "4         0         0         0         0         0     1            0  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno','year','gndr'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies as well as 'year' and 'gndr' into binary categoricals\n",
    "X = pd.concat([X, pd.get_dummies(df[['cntry']])], axis=1)\n",
    "X['male'] = np.where(df[\"gndr\"] == 2, 1, 0)\n",
    "X['second_year'] = np.where(df[\"year\"] == 7, 1, 0)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the correlation below, the second highest (and highest *positive*) one-to-one variable correlation with the partner-target variable is with 'sclmeet', their level of social interaction. Let's dig a little further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing correlation effect, squaring this feature increases the correlation percentage by over 1 percent. \n",
    "X['sclmeet_squared'] = X.sclmeet ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observing age affect on target\n",
    "X['target'] = y\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.countplot(x='agea', hue='target', data=X)\n",
    "plt.show()\n",
    "\n",
    "X = X.drop('target', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9521158129175946"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating categorical variable for the high-target age group below 23\n",
    "X['younger_than_23'] = np.where(X.agea < 23, 1, 0)\n",
    "y[X.younger_than_23 == 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['age_times_happy'] = X.agea * X.happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tvtot</th>\n",
       "      <th>ppltrst</th>\n",
       "      <th>pplfair</th>\n",
       "      <th>pplhlp</th>\n",
       "      <th>happy</th>\n",
       "      <th>sclmeet</th>\n",
       "      <th>sclact</th>\n",
       "      <th>agea</th>\n",
       "      <th>cntry_CH</th>\n",
       "      <th>cntry_CZ</th>\n",
       "      <th>cntry_DE</th>\n",
       "      <th>cntry_ES</th>\n",
       "      <th>cntry_NO</th>\n",
       "      <th>cntry_SE</th>\n",
       "      <th>male</th>\n",
       "      <th>second_year</th>\n",
       "      <th>sclmeet_squared</th>\n",
       "      <th>younger_than_23</th>\n",
       "      <th>age_times_happy</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tvtot</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.142422</td>\n",
       "      <td>-0.119277</td>\n",
       "      <td>-0.069080</td>\n",
       "      <td>-0.118598</td>\n",
       "      <td>-0.078864</td>\n",
       "      <td>-0.092375</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>-0.173825</td>\n",
       "      <td>0.218606</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.035985</td>\n",
       "      <td>-0.017619</td>\n",
       "      <td>-0.050075</td>\n",
       "      <td>0.017922</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>-0.077242</td>\n",
       "      <td>-0.109670</td>\n",
       "      <td>0.159138</td>\n",
       "      <td>-0.028816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppltrst</th>\n",
       "      <td>-0.142422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.459250</td>\n",
       "      <td>0.231533</td>\n",
       "      <td>0.122555</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>0.031936</td>\n",
       "      <td>-0.225021</td>\n",
       "      <td>-0.009180</td>\n",
       "      <td>-0.152500</td>\n",
       "      <td>0.217530</td>\n",
       "      <td>0.132684</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>0.113109</td>\n",
       "      <td>0.012685</td>\n",
       "      <td>0.093452</td>\n",
       "      <td>-0.034371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplfair</th>\n",
       "      <td>-0.119277</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.480931</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.058250</td>\n",
       "      <td>-0.215494</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>-0.188744</td>\n",
       "      <td>0.191488</td>\n",
       "      <td>0.161542</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.083597</td>\n",
       "      <td>0.018556</td>\n",
       "      <td>0.142029</td>\n",
       "      <td>-0.034054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplhlp</th>\n",
       "      <td>-0.069080</td>\n",
       "      <td>0.459250</td>\n",
       "      <td>0.480931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.215323</td>\n",
       "      <td>0.080489</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>0.079003</td>\n",
       "      <td>-0.166372</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.223920</td>\n",
       "      <td>0.153183</td>\n",
       "      <td>0.173921</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>-0.032901</td>\n",
       "      <td>0.072531</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.146244</td>\n",
       "      <td>-0.022069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>-0.118598</td>\n",
       "      <td>0.231533</td>\n",
       "      <td>0.247755</td>\n",
       "      <td>0.215323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182944</td>\n",
       "      <td>0.192030</td>\n",
       "      <td>-0.042970</td>\n",
       "      <td>0.110593</td>\n",
       "      <td>-0.206445</td>\n",
       "      <td>-0.003884</td>\n",
       "      <td>-0.074060</td>\n",
       "      <td>0.100777</td>\n",
       "      <td>0.063745</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>-0.013378</td>\n",
       "      <td>0.172120</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>0.473904</td>\n",
       "      <td>-0.145061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sclmeet</th>\n",
       "      <td>-0.078864</td>\n",
       "      <td>0.122555</td>\n",
       "      <td>0.096501</td>\n",
       "      <td>0.080489</td>\n",
       "      <td>0.182944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.283319</td>\n",
       "      <td>-0.194443</td>\n",
       "      <td>-0.021595</td>\n",
       "      <td>-0.174738</td>\n",
       "      <td>-0.022014</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>0.097158</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>-0.028502</td>\n",
       "      <td>0.985043</td>\n",
       "      <td>0.192910</td>\n",
       "      <td>-0.075821</td>\n",
       "      <td>0.162970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sclact</th>\n",
       "      <td>-0.092375</td>\n",
       "      <td>0.137491</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.192030</td>\n",
       "      <td>0.283319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.059442</td>\n",
       "      <td>-0.004271</td>\n",
       "      <td>-0.038131</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>-0.107427</td>\n",
       "      <td>0.080589</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>0.268370</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>0.053886</td>\n",
       "      <td>0.011405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agea</th>\n",
       "      <td>0.257674</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>-0.042970</td>\n",
       "      <td>-0.194443</td>\n",
       "      <td>-0.059442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015258</td>\n",
       "      <td>-0.033002</td>\n",
       "      <td>0.030541</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>0.048733</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>-0.200770</td>\n",
       "      <td>-0.546374</td>\n",
       "      <td>0.838211</td>\n",
       "      <td>-0.256670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cntry_CH</th>\n",
       "      <td>-0.173825</td>\n",
       "      <td>0.031936</td>\n",
       "      <td>0.058250</td>\n",
       "      <td>0.079003</td>\n",
       "      <td>0.110593</td>\n",
       "      <td>-0.021595</td>\n",
       "      <td>-0.004271</td>\n",
       "      <td>-0.015258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.196084</td>\n",
       "      <td>-0.027113</td>\n",
       "      <td>-0.294179</td>\n",
       "      <td>-0.216024</td>\n",
       "      <td>-0.243774</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>-0.035229</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>-0.019266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cntry_CZ</th>\n",
       "      <td>0.218606</td>\n",
       "      <td>-0.225021</td>\n",
       "      <td>-0.215494</td>\n",
       "      <td>-0.166372</td>\n",
       "      <td>-0.206445</td>\n",
       "      <td>-0.174738</td>\n",
       "      <td>-0.038131</td>\n",
       "      <td>-0.033002</td>\n",
       "      <td>-0.196084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>-0.260926</td>\n",
       "      <td>-0.191605</td>\n",
       "      <td>-0.216219</td>\n",
       "      <td>0.024788</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>-0.168655</td>\n",
       "      <td>-0.004459</td>\n",
       "      <td>-0.137669</td>\n",
       "      <td>0.033158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cntry_DE</th>\n",
       "      <td>0.004144</td>\n",
       "      <td>-0.009180</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.003884</td>\n",
       "      <td>-0.022014</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.030541</td>\n",
       "      <td>-0.027113</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.036078</td>\n",
       "      <td>-0.026493</td>\n",
       "      <td>-0.029897</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>-0.027919</td>\n",
       "      <td>-0.020296</td>\n",
       "      <td>0.024773</td>\n",
       "      <td>-0.014865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cntry_ES</th>\n",
       "      <td>0.035985</td>\n",
       "      <td>-0.152500</td>\n",
       "      <td>-0.188744</td>\n",
       "      <td>-0.223920</td>\n",
       "      <td>-0.074060</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>-0.107427</td>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.294179</td>\n",
       "      <td>-0.260926</td>\n",
       "      <td>-0.036078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.287460</td>\n",
       "      <td>-0.324387</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>-0.006935</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>-0.030519</td>\n",
       "      <td>-0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cntry_NO</th>\n",
       "      <td>-0.017619</td>\n",
       "      <td>0.217530</td>\n",
       "      <td>0.191488</td>\n",
       "      <td>0.153183</td>\n",
       "      <td>0.100777</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>0.080589</td>\n",
       "      <td>-0.023270</td>\n",
       "      <td>-0.216024</td>\n",
       "      <td>-0.191605</td>\n",
       "      <td>-0.026493</td>\n",
       "      <td>-0.287460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.238206</td>\n",
       "      <td>-0.027735</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.066682</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.033952</td>\n",
       "      <td>-0.014130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cntry_SE</th>\n",
       "      <td>-0.050075</td>\n",
       "      <td>0.132684</td>\n",
       "      <td>0.161542</td>\n",
       "      <td>0.173921</td>\n",
       "      <td>0.063745</td>\n",
       "      <td>0.097158</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.048733</td>\n",
       "      <td>-0.243774</td>\n",
       "      <td>-0.216219</td>\n",
       "      <td>-0.029897</td>\n",
       "      <td>-0.324387</td>\n",
       "      <td>-0.238206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019075</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.102509</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.079763</td>\n",
       "      <td>0.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.017922</td>\n",
       "      <td>-0.029921</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.024788</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>-0.027735</td>\n",
       "      <td>-0.019075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>-0.008426</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.033984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second_year</th>\n",
       "      <td>-0.031596</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.032901</td>\n",
       "      <td>-0.013378</td>\n",
       "      <td>-0.028502</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>-0.003520</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>-0.006935</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035147</td>\n",
       "      <td>0.020949</td>\n",
       "      <td>-0.009733</td>\n",
       "      <td>0.018689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sclmeet_squared</th>\n",
       "      <td>-0.077242</td>\n",
       "      <td>0.113109</td>\n",
       "      <td>0.083597</td>\n",
       "      <td>0.072531</td>\n",
       "      <td>0.172120</td>\n",
       "      <td>0.985043</td>\n",
       "      <td>0.268370</td>\n",
       "      <td>-0.200770</td>\n",
       "      <td>-0.035229</td>\n",
       "      <td>-0.168655</td>\n",
       "      <td>-0.027919</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.066682</td>\n",
       "      <td>0.102509</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>-0.035147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.206167</td>\n",
       "      <td>-0.088282</td>\n",
       "      <td>0.176098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younger_than_23</th>\n",
       "      <td>-0.109670</td>\n",
       "      <td>0.012685</td>\n",
       "      <td>0.018556</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>0.192910</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>-0.546374</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>-0.004459</td>\n",
       "      <td>-0.020296</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>0.027359</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>-0.008426</td>\n",
       "      <td>0.020949</td>\n",
       "      <td>0.206167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.448794</td>\n",
       "      <td>0.410501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_times_happy</th>\n",
       "      <td>0.159138</td>\n",
       "      <td>0.093452</td>\n",
       "      <td>0.142029</td>\n",
       "      <td>0.146244</td>\n",
       "      <td>0.473904</td>\n",
       "      <td>-0.075821</td>\n",
       "      <td>0.053886</td>\n",
       "      <td>0.838211</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>-0.137669</td>\n",
       "      <td>0.024773</td>\n",
       "      <td>-0.030519</td>\n",
       "      <td>0.033952</td>\n",
       "      <td>0.079763</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>-0.009733</td>\n",
       "      <td>-0.088282</td>\n",
       "      <td>-0.448794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.305113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>-0.028816</td>\n",
       "      <td>-0.034371</td>\n",
       "      <td>-0.034054</td>\n",
       "      <td>-0.022069</td>\n",
       "      <td>-0.145061</td>\n",
       "      <td>0.162970</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>-0.256670</td>\n",
       "      <td>-0.019266</td>\n",
       "      <td>0.033158</td>\n",
       "      <td>-0.014865</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>-0.014130</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.018689</td>\n",
       "      <td>0.176098</td>\n",
       "      <td>0.410501</td>\n",
       "      <td>-0.305113</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tvtot   ppltrst   pplfair    pplhlp     happy   sclmeet  \\\n",
       "tvtot            1.000000 -0.142422 -0.119277 -0.069080 -0.118598 -0.078864   \n",
       "ppltrst         -0.142422  1.000000  0.597506  0.459250  0.231533  0.122555   \n",
       "pplfair         -0.119277  0.597506  1.000000  0.480931  0.247755  0.096501   \n",
       "pplhlp          -0.069080  0.459250  0.480931  1.000000  0.215323  0.080489   \n",
       "happy           -0.118598  0.231533  0.247755  0.215323  1.000000  0.182944   \n",
       "sclmeet         -0.078864  0.122555  0.096501  0.080489  0.182944  1.000000   \n",
       "sclact          -0.092375  0.137491  0.128808  0.092673  0.192030  0.283319   \n",
       "agea             0.257674 -0.029412  0.014724  0.040351 -0.042970 -0.194443   \n",
       "cntry_CH        -0.173825  0.031936  0.058250  0.079003  0.110593 -0.021595   \n",
       "cntry_CZ         0.218606 -0.225021 -0.215494 -0.166372 -0.206445 -0.174738   \n",
       "cntry_DE         0.004144 -0.009180  0.005904  0.003274 -0.003884 -0.022014   \n",
       "cntry_ES         0.035985 -0.152500 -0.188744 -0.223920 -0.074060  0.011601   \n",
       "cntry_NO        -0.017619  0.217530  0.191488  0.153183  0.100777  0.070484   \n",
       "cntry_SE        -0.050075  0.132684  0.161542  0.173921  0.063745  0.097158   \n",
       "male             0.017922 -0.029921  0.022251  0.042046 -0.022413  0.009533   \n",
       "second_year     -0.031596 -0.001600  0.004283 -0.032901 -0.013378 -0.028502   \n",
       "sclmeet_squared -0.077242  0.113109  0.083597  0.072531  0.172120  0.985043   \n",
       "younger_than_23 -0.109670  0.012685  0.018556  0.021953  0.064411  0.192910   \n",
       "age_times_happy  0.159138  0.093452  0.142029  0.146244  0.473904 -0.075821   \n",
       "target          -0.028816 -0.034371 -0.034054 -0.022069 -0.145061  0.162970   \n",
       "\n",
       "                   sclact      agea  cntry_CH  cntry_CZ  cntry_DE  cntry_ES  \\\n",
       "tvtot           -0.092375  0.257674 -0.173825  0.218606  0.004144  0.035985   \n",
       "ppltrst          0.137491 -0.029412  0.031936 -0.225021 -0.009180 -0.152500   \n",
       "pplfair          0.128808  0.014724  0.058250 -0.215494  0.005904 -0.188744   \n",
       "pplhlp           0.092673  0.040351  0.079003 -0.166372  0.003274 -0.223920   \n",
       "happy            0.192030 -0.042970  0.110593 -0.206445 -0.003884 -0.074060   \n",
       "sclmeet          0.283319 -0.194443 -0.021595 -0.174738 -0.022014  0.011601   \n",
       "sclact           1.000000 -0.059442 -0.004271 -0.038131  0.003764 -0.107427   \n",
       "agea            -0.059442  1.000000 -0.015258 -0.033002  0.030541  0.010583   \n",
       "cntry_CH        -0.004271 -0.015258  1.000000 -0.196084 -0.027113 -0.294179   \n",
       "cntry_CZ        -0.038131 -0.033002 -0.196084  1.000000 -0.024048 -0.260926   \n",
       "cntry_DE         0.003764  0.030541 -0.027113 -0.024048  1.000000 -0.036078   \n",
       "cntry_ES        -0.107427  0.010583 -0.294179 -0.260926 -0.036078  1.000000   \n",
       "cntry_NO         0.080589 -0.023270 -0.216024 -0.191605 -0.026493 -0.287460   \n",
       "cntry_SE         0.080040  0.048733 -0.243774 -0.216219 -0.029897 -0.324387   \n",
       "male            -0.031648  0.020598  0.013925  0.024788  0.011096  0.007807   \n",
       "second_year      0.012150 -0.003520  0.003218  0.001472  0.002100 -0.006935   \n",
       "sclmeet_squared  0.268370 -0.200770 -0.035229 -0.168655 -0.027919  0.017570   \n",
       "younger_than_23  0.064411 -0.546374  0.017730 -0.004459 -0.020296 -0.038907   \n",
       "age_times_happy  0.053886  0.838211  0.040859 -0.137669  0.024773 -0.030519   \n",
       "target           0.011405 -0.256670 -0.019266  0.033158 -0.014865 -0.000387   \n",
       "\n",
       "                 cntry_NO  cntry_SE      male  second_year  sclmeet_squared  \\\n",
       "tvtot           -0.017619 -0.050075  0.017922    -0.031596        -0.077242   \n",
       "ppltrst          0.217530  0.132684 -0.029921    -0.001600         0.113109   \n",
       "pplfair          0.191488  0.161542  0.022251     0.004283         0.083597   \n",
       "pplhlp           0.153183  0.173921  0.042046    -0.032901         0.072531   \n",
       "happy            0.100777  0.063745 -0.022413    -0.013378         0.172120   \n",
       "sclmeet          0.070484  0.097158  0.009533    -0.028502         0.985043   \n",
       "sclact           0.080589  0.080040 -0.031648     0.012150         0.268370   \n",
       "agea            -0.023270  0.048733  0.020598    -0.003520        -0.200770   \n",
       "cntry_CH        -0.216024 -0.243774  0.013925     0.003218        -0.035229   \n",
       "cntry_CZ        -0.191605 -0.216219  0.024788     0.001472        -0.168655   \n",
       "cntry_DE        -0.026493 -0.029897  0.011096     0.002100        -0.027919   \n",
       "cntry_ES        -0.287460 -0.324387  0.007807    -0.006935         0.017570   \n",
       "cntry_NO         1.000000 -0.238206 -0.027735     0.001012         0.066682   \n",
       "cntry_SE        -0.238206  1.000000 -0.019075     0.002085         0.102509   \n",
       "male            -0.027735 -0.019075  1.000000     0.001355         0.010007   \n",
       "second_year      0.001012  0.002085  0.001355     1.000000        -0.035147   \n",
       "sclmeet_squared  0.066682  0.102509  0.010007    -0.035147         1.000000   \n",
       "younger_than_23  0.027359  0.007436 -0.008426     0.020949         0.206167   \n",
       "age_times_happy  0.033952  0.079763  0.002397    -0.009733        -0.088282   \n",
       "target          -0.014130  0.004964  0.033984     0.018689         0.176098   \n",
       "\n",
       "                 younger_than_23  age_times_happy    target  \n",
       "tvtot                  -0.109670         0.159138 -0.028816  \n",
       "ppltrst                 0.012685         0.093452 -0.034371  \n",
       "pplfair                 0.018556         0.142029 -0.034054  \n",
       "pplhlp                  0.021953         0.146244 -0.022069  \n",
       "happy                   0.064411         0.473904 -0.145061  \n",
       "sclmeet                 0.192910        -0.075821  0.162970  \n",
       "sclact                  0.064411         0.053886  0.011405  \n",
       "agea                   -0.546374         0.838211 -0.256670  \n",
       "cntry_CH                0.017730         0.040859 -0.019266  \n",
       "cntry_CZ               -0.004459        -0.137669  0.033158  \n",
       "cntry_DE               -0.020296         0.024773 -0.014865  \n",
       "cntry_ES               -0.038907        -0.030519 -0.000387  \n",
       "cntry_NO                0.027359         0.033952 -0.014130  \n",
       "cntry_SE                0.007436         0.079763  0.004964  \n",
       "male                   -0.008426         0.002397  0.033984  \n",
       "second_year             0.020949        -0.009733  0.018689  \n",
       "sclmeet_squared         0.206167        -0.088282  0.176098  \n",
       "younger_than_23         1.000000        -0.448794  0.410501  \n",
       "age_times_happy        -0.448794         1.000000 -0.305113  \n",
       "target                  0.410501        -0.305113  1.000000  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['target'] = y\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('target', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting-Prevention through Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = int(X.shape[0] * 0.9)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77205882 0.73284314 0.71446078 0.72883436 0.77395577 0.76904177\n",
      " 0.73341523 0.74815725 0.77272727 0.74324324]\n",
      "0.74887376414668\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of the new feature ('agea_times_happy') increases the accuracy by 0.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Iterations\n",
    "##### 750 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=750,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting number of estimators from 500 to 750\n",
    "params = {'n_estimators': 750,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77205882 0.73651961 0.70833333 0.72760736 0.77149877 0.77149877\n",
      " 0.73341523 0.74570025 0.76658477 0.74570025]\n",
      "0.7478917161067107\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1000 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting number of estimators from 500 to 1000\n",
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76960784 0.73529412 0.70955882 0.73128834 0.77027027 0.77027027\n",
      " 0.74201474 0.72113022 0.76289926 0.73955774]\n",
      "0.7451891634014516\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is greater variance in the scores, strongly suggesting overfitting for certain subsets and causing problems in others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='exponential', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting loss function from 'deviance' to 'exponential'\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'exponential'}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76838235 0.70588235 0.71323529 0.72392638 0.77395577 0.77027027\n",
      " 0.73587224 0.74324324 0.77149877 0.74570025]\n",
      "0.7451966920908639\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function adjustment doesn't seem to affect the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting Weak-Learner Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting max-depth from '2' to '3'\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 3,\n",
    "          'loss': 'deviance'}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75980392 0.71691176 0.69240196 0.72515337 0.77027027 0.76658477\n",
      " 0.73955774 0.65356265 0.74570025 0.74692875]\n",
      "0.7316875443896376\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjusting max-depth from '2' to '4'\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'loss': 'deviance'}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77328431 0.71078431 0.67892157 0.72515337 0.75921376 0.75552826\n",
      " 0.73710074 0.57739558 0.74938575 0.74324324]\n",
      "0.7210010892178882\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 'warm_start' parameter\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'warm_start': True,\n",
    "          'loss': 'deviance'}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77205882 0.73284314 0.71446078 0.72883436 0.77395577 0.76904177\n",
      " 0.73341523 0.74815725 0.77149877 0.74324324]\n",
      "0.74875091402383\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adjusting the different settings and observing their results on the accuracy of the model on the test set, the results showed that the only adjustment that seemed to make a considerable difference was in the creation of new features."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "59px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
