{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Credit Card Fraud with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary View of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      124592\n",
       "V1        275663\n",
       "V2        275663\n",
       "V3        275663\n",
       "V4        275663\n",
       "V5        275663\n",
       "V6        275663\n",
       "V7        275663\n",
       "V8        275663\n",
       "V9        275663\n",
       "V10       275663\n",
       "V11       275663\n",
       "V12       275663\n",
       "V13       275663\n",
       "V14       275663\n",
       "V15       275663\n",
       "V16       275663\n",
       "V17       275663\n",
       "V18       275663\n",
       "V19       275663\n",
       "V20       275663\n",
       "V21       275663\n",
       "V22       275663\n",
       "V23       275663\n",
       "V24       275663\n",
       "V25       275663\n",
       "V26       275663\n",
       "V27       275663\n",
       "V28       275663\n",
       "Amount     32767\n",
       "Class          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001727485630620034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1905d0fd470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.Class.sum() / len(df.Class))\n",
    "sns.countplot(df.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>norm_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Class  norm_amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053      0     0.244964  \n",
       "1  0.167170  0.125895 -0.008983  0.014724      0    -0.342475  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752      0     1.160686  \n",
       "3  0.647376 -0.221929  0.062723  0.061458      0     0.140534  \n",
       "4 -0.206010  0.502292  0.219422  0.215153      0    -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['norm_amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "df = df.drop(['Time','Amount'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Our Dataset for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Class', 1)\n",
    "y = df.Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the \"Class\" target variable is skewed and fraud is under-represented in this set, we will use traditional undersampling on this dataset to create non-skewed subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  984\n"
     ]
    }
   ],
   "source": [
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(df[df.Class == 1])\n",
    "fraud_indices = np.array(df[df.Class == 1].index)\n",
    "normal_indices = df[df.Class == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = df.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = under_sample_data.loc[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.loc[:, under_sample_data.columns == 'Class']\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions in train dataset:  199364\n",
      "Number transactions in test dataset:  85443\n",
      "Total number of transactions:  284807\n",
      "\n",
      "Number transactions in train undersample dataset:  688\n",
      "Number transactions in test undersample dataset:  296\n",
      "Total number of transactions:  984\n"
     ]
    }
   ],
   "source": [
    "# Whole dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Number transactions in train dataset: \", len(X_train))\n",
    "print(\"Number transactions in test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
    "\n",
    "# Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
    "                                                                                                   ,y_undersample\n",
    "                                                                                                   ,test_size = 0.3\n",
    "                                                                                                   ,random_state = 0)\n",
    "print()\n",
    "print(\"Number transactions in train undersample dataset: \", len(X_train_undersample))\n",
    "print(\"Number transactions in test undersample dataset: \", len(X_test_undersample))\n",
    "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Model\n",
    "The assignment prioritizes correctly finding fraud rather than correctly labeling non-fraudulent transactions. The reasoning? If we predict that a transaction is fraudulent and it turns out not to be (false positive), this is not a massive problem compared to the opposite of missing an actual fradulent transaction (true positive). Therefore, we are most interested in calculating recall.\n",
    "* Accuracy = (TP+TN) / Total\n",
    "* Precision = TP / (TP+FP)\n",
    "* Recall = TP / (TP+FN).\n",
    "\n",
    "Trying to increase recall tends to cause a decrease in precision. However, as mentioned earlier, this is a worthy trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=.01, penalty= 'l1')\n",
    "lr.fit(X_train_undersample, y_train_undersample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9319727891156463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[130,  19],\n",
       "       [ 10, 137]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_undersample = lr.predict(X_test_undersample)  \n",
    "\n",
    "print(recall_score(y_test_undersample,y_pred_undersample))\n",
    "confusion_matrix(y_test_undersample, y_pred_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the model on the undersampled test set with a 50/50 split of target variables, the model performs reasonably well. Let us continue moving forward by testing on the dataset as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall metric in the testing dataset:  0.9183673469387755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1905df1f4a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LogisticRegression(C = .01, penalty = 'l1')\n",
    "lr.fit(X_train_undersample, y_train_undersample)\n",
    "y_pred = lr.predict(X_test.values)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "sns.heatmap(cnf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After fitting the model to the undersampled set and applying the model to the actual test dataset, the model still performed reasonably well at 92.5%. This model could still be improved by increasing its sensitivity to recall (at the expense of precision). Considering that the group of fraud-positive target variables is not very large, we will settle for this score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
